{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/alexisrajab/opt/anaconda3/lib/python3.8/site-packages (1.24.2)\n",
      "Requirement already satisfied: pandas in /Users/alexisrajab/opt/anaconda3/lib/python3.8/site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/alexisrajab/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/alexisrajab/opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/alexisrajab/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alexisrajab/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy --user\n",
    "!pip install pandas --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexisrajab/Desktop/3A/SM10/RL/Cryptocurrency-Trading-bot-using-RL\n"
     ]
    }
   ],
   "source": [
    "cd /Users/alexisrajab/Desktop/3A/SM10/RL/Cryptocurrency-Trading-bot-using-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "from RLGlue.rl_glue import RLGlue\n",
    "\n",
    "from src.environment import Environment\n",
    "from src.agents import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RLGlue.environment import BaseEnvironment\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class EnvironmentREINFORCE(BaseEnvironment):\n",
    "    \"\"\"Implements the environment for an RLGlue environment\n",
    "    Note:\n",
    "        env_init, env_start, env_step, env_cleanup, and env_message are required\n",
    "        methods.\n",
    "    \"\"\"\n",
    "\n",
    "    actions = [0]\n",
    "\n",
    "    def __init__(self):\n",
    "        reward = None\n",
    "        observation = None\n",
    "        termination = None\n",
    "        self.reward_obs_term = (reward, observation, termination)\n",
    "        self.count = 0\n",
    "        self.data = None\n",
    "        self.seed = None\n",
    "        self.time = 0\n",
    "        self.max_time = None\n",
    "\n",
    "    def get_full_obs(self):\n",
    "        infos = ['weighted_positive_score','weighted_neutral_score','weighted_negative_score',\n",
    "        'total', 'Bitcoin','BTC','BNB','ETH']\n",
    "\n",
    "        return [self.data[key][self.time] for key in infos]\n",
    "    \n",
    "    def get_obs(self):\n",
    "        infos = ['weighted_positive_score','weighted_neutral_score','weighted_negative_score',\n",
    "        'total']\n",
    "\n",
    "        return [self.data[key][self.time] for key in infos]\n",
    "    \n",
    "    def env_init(self, env_info={}):\n",
    "        \"\"\"Setup for the environment called when the experiment first starts.\n",
    "        Note:\n",
    "            Initialize a tuple with the reward, first state observation, boolean\n",
    "            indicating if it's terminal.\n",
    "        \"\"\"\n",
    "        self.max_time= env_info['max']\n",
    "        self.data = env_info['data']\n",
    "        local_observation = []\n",
    "        local_observation = self.get_full_obs()\n",
    "        self.time +=1\n",
    "\n",
    "        self.reward_obs_term = (0.0, local_observation,False)\n",
    "\n",
    "\n",
    "\n",
    "    def env_start(self):\n",
    "        \"\"\"The first method called when the experiment starts, called before the\n",
    "        agent starts.\n",
    "        Returns:\n",
    "            The first state observation from the environment. Gives 5 points to agent\n",
    "        \"\"\"\n",
    "        return self.reward_obs_term\n",
    "    \n",
    "    def NUPL(self,portfolio,current):\n",
    "        \"\"\"Method to get NUPLS fro each crypto according to agent portfolio\n",
    "        Args:\n",
    "            portfolio : agent portfolio\n",
    "            current : the current market value for each crypto\n",
    "        Returns:\n",
    "            NUPLs\n",
    "        \"\"\"\n",
    "        NUPLs = []\n",
    "        for i,crypto in enumerate(['Bitcoin','BTC','BNB','ETH']):\n",
    "\n",
    "            total = sum([nb for nb , value in portfolio[crypto]])\n",
    "            if portfolio[crypto] != []:\n",
    "                NUPLs.append(sum([(current[i] - value)*nb for nb , value in portfolio[crypto]]) / total*current[i])\n",
    "            else:\n",
    "                NUPLs.append(0)\n",
    "        return NUPLs\n",
    "    \n",
    "    def update_agent_portfolio(self,market_values,action,portfolio,cash):\n",
    "        \"\"\"Method update virtually the agent portfolio according to the action he chose\n",
    "        Args:\n",
    "            portfolio : agent portfolio\n",
    "            market_values : the current market value for each crypto\n",
    "            action: the actions taken by the agent \n",
    "            cash : agent cash for teh buy action\n",
    "        Returns:\n",
    "            portfolio \n",
    "        \"\"\"\n",
    "        for i,crypto in enumerate(['Bitcoin','BTC','BNB','ETH']):\n",
    "            if action[i] ==1:\n",
    "                portfolio[crypto].append((cash/market_values[i] ,market_values[i]))\n",
    "            if action[i] == 2:\n",
    "                portfolio[crypto] = []\n",
    "        return portfolio\n",
    "    \n",
    "    def env_step(self, action):\n",
    "        \"\"\"A step taken by the environment.\n",
    "        Args:\n",
    "            action: The action taken by the agent\n",
    "        Returns:\n",
    "            (float, state, Boolean): a tuple of the reward, state observation,\n",
    "                and boolean indicating if it's terminal.\n",
    "        \"\"\"\n",
    "        current = self.get_full_obs()[-4:][1] # get the value the agent tried to predict # BTC : index 1\n",
    "        #portfolio , action_per_crypto , cash = action\n",
    "        #portfolio = self.update_agent_portfolio(current,action_per_crypto,portfolio,cash) # update by the agent\n",
    "        \n",
    "        #reward = sum(self.NUPL(portfolio,current))\n",
    "        self.time += 1\n",
    "\n",
    "        obs = self.get_obs()\n",
    "        \n",
    "        if self.time != self.max_time:\n",
    "            #self.reward_obs_term = (reward,(obs, current), False)\n",
    "            self.reward_obs_term = (0,current, False) # the reward is computed afterwards\n",
    "            \n",
    "        else:\n",
    "            #self.reward_obs_term = (reward, (obs, current), True)\n",
    "            self.reward_obs_term = (0,current,True)\n",
    "            \n",
    "        return self.reward_obs_term\n",
    "\n",
    "    def env_cleanup(self):\n",
    "        \"\"\"Cleanup done after the environment ends\"\"\"\n",
    "        pass\n",
    "\n",
    "    def env_message(self, message):\n",
    "        \"\"\"A message asking the environment for information\n",
    "        Args:\n",
    "            message (string): the message passed to the environment\n",
    "        Returns:\n",
    "            string: the response (or answer) to the message\n",
    "        \"\"\"\n",
    "        if message == \"what is the current reward?\":\n",
    "            return \"{}\".format(self.reward_obs_term[0])\n",
    "\n",
    "        # else\n",
    "        return \"I don't know how to respond to your message\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_csv(\"data/predictions_with_sentiment_BTC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51132.4609375"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[\"Predicted_price_without_sentiment\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "s_size = 3 # [current,predicted_future,bitcoin_value]\n",
    "a_size = 3 # [buy,hold,sell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hold = 0, sell = 1, buy = 2\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \n",
    "    ### RL_glue agent functions\n",
    "    \n",
    "    def agent_init(self, agent_info={}):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
    "        self.cash = agent_info['cash'] # total cash of the agent \n",
    "        #self.model = agent_info[\"model\"] # model for prediction\n",
    "        self.crypto_name = agent_info['crypto'] #name of the considered crypto\n",
    "        self.crypto_value = 0\n",
    "        self.crypto_number = 0\n",
    "        self.portfolio = {self.crypto_name : [self.crypto_number,self.crypto_value]}\n",
    "        \n",
    "        self.log_prob = []\n",
    "        \n",
    "        self.crypto_predicted = 0\n",
    "        self.mode = \"train\" # change it to \"test\" when testing the agent\n",
    "        \n",
    "    def agent_start(self):\n",
    "        return 0,self.cash # start by holding\n",
    "    \n",
    "    def agent_step(self,reward,observation):\n",
    "        \n",
    "        state = [observation,self.crypto_predicted,self.crypto_value]\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            action,log_prob = act(state)\n",
    "            self.log_prob.append(log_prob)\n",
    "            return action\n",
    "        \n",
    "        if self.mode == \"test\":\n",
    "            return greedy_act(state)\n",
    "    \n",
    "    def agent_end(self,reward):\n",
    "        #####\n",
    "        return None\n",
    "    \n",
    "    def update_portfolio(self,action,current):\n",
    "        \n",
    "        if action == 1:\n",
    "            self.cash +=  self.portfolio[self.crypto_name][0]*current\n",
    "            self.portfolio[self.crypto_name][0] = 0\n",
    "            self.portfolio[self.crypto_name][1] = 0\n",
    "        if action == 2:\n",
    "            if self.cash > 0:\n",
    "                self.portfolio[self.crypto_name][0] += self.cash/current\n",
    "                self.portfolio[self.crypto_name][1] = current\n",
    "        \n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        # define the network layers.\n",
    "        # here we have a single hidden layer with h_size neurons\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define the forward pass\n",
    "        # here we use ReLU activation for the hidden layer\n",
    "        # and softmax activation for the output layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        # sample an action from the policy network\n",
    "        # and return the log probability of that action\n",
    "        # this will be used in the loss function\n",
    "        state = torch.tensor(np.array([state])).to(device)\n",
    "        print(state)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "\n",
    "    def greedy_act(self,state):\n",
    "        # takes the observation and returns the greedy action\n",
    "        # this will be used to evaluate the policy\n",
    "        state = torch.tensor(np.array([state]),dtype=torch.float).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        return torch.argmax(probs).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hold = 0, sell = 1, buy = 2\n",
    "\n",
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=50)\n",
    "    scores = []\n",
    "    \n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        \n",
    "        i = 1\n",
    "        \n",
    "        env_info = {'max' : num_obs , 'data':data}\n",
    "        agent_info = {'cash':100000, 'crypto': ['BTC']}\n",
    "        \n",
    "        env = EnvironmentREINFORCE\n",
    "        agent = policy\n",
    "        \n",
    "        rl_glue = RLGlue(env, agent)  # Creates a new RLGlue experiment with the env and agent we chose \n",
    "        rl_glue.rl_init(agent_info, env_info) # Pass RLGlue what it needs to initialize the agent and environment\n",
    "        rl_glue.rl_start() \n",
    "                      \n",
    "        # generate an episode\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        policy = saved_log_probs\n",
    "        \n",
    "        \n",
    "        for t in range(max_t):\n",
    "                      \n",
    "            i=+1\n",
    "            policy.crypto_predicted = pred[\"Predicted_price_without_sentiment\"][i]\n",
    "            \n",
    "            reward, obs, action, done = rl_glue.rl_step() # env_step() and agent_step() are called\n",
    "            saved_log_probs = policy.log_probs # updated during agent_step\n",
    "                      \n",
    "            # obs = current bitcoin value\n",
    "                      \n",
    "            policy.update_portfolio(action,obs)\n",
    "            \n",
    "            if policy.crypto_number == 0:      \n",
    "                reward = 0\n",
    "                      \n",
    "            else:\n",
    "                reward = (obs-policy.crypto_value)/obs # update the nupl value\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        \n",
    "        # save the score\n",
    "\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        \n",
    "        returns = deque(maxlen=max_t) \n",
    "        n_steps = len(rewards) \n",
    "\n",
    "        # calculate the discounted returns for each step\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft( gamma*disc_return_t + rewards[t]   )    \n",
    "            \n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        ## eps is the smallest representable float, which is \n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities        \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        # compute the loss function to be minimized\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # update the policy network with the backward pass\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print the average score every 100 episodes\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_hyperparameters = {\n",
    "    \"h_size\": 16,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 100,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": \"trader\",\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m num_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train the policy\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mreinforce\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                   \u001b[49m\u001b[43magent_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                   \u001b[49m\u001b[43magent_hyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_training_episodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                   \u001b[49m\u001b[43magent_hyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_t\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                   \u001b[49m\u001b[43magent_hyperparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 18\u001b[0m, in \u001b[0;36mreinforce\u001b[0;34m(policy, optimizer, n_training_episodes, max_t, gamma, print_every)\u001b[0m\n\u001b[1;32m     15\u001b[0m env \u001b[38;5;241m=\u001b[39m EnvironmentREINFORCE\n\u001b[1;32m     16\u001b[0m agent \u001b[38;5;241m=\u001b[39m policy\n\u001b[0;32m---> 18\u001b[0m rl_glue \u001b[38;5;241m=\u001b[39m \u001b[43mRLGlue\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Creates a new RLGlue experiment with the env and agent we chose \u001b[39;00m\n\u001b[1;32m     19\u001b[0m rl_glue\u001b[38;5;241m.\u001b[39mrl_init(agent_info, env_info) \u001b[38;5;66;03m# Pass RLGlue what it needs to initialize the agent and environment\u001b[39;00m\n\u001b[1;32m     20\u001b[0m rl_glue\u001b[38;5;241m.\u001b[39mrl_start() \n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/RLGlue/rl_glue.py:17\u001b[0m, in \u001b[0;36mRLGlue.__init__\u001b[0;34m(self, env_class, agent_class)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_class, agent_class):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment \u001b[38;5;241m=\u001b[39m env_class()\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent \u001b[38;5;241m=\u001b[39m \u001b[43magent_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "# Create policy and place it to the device\n",
    "from collections import deque\n",
    "agent_policy = Policy(agent_hyperparameters[\"state_space\"], agent_hyperparameters[\"action_space\"], agent_hyperparameters[\"h_size\"]).to(device)\n",
    "agent_optimizer = optim.Adam(agent_policy.parameters(), lr=agent_hyperparameters[\"lr\"])\n",
    "\n",
    "data = pd.read_csv(\"./data/all_data.csv\").to_dict() # full data with price and sentiment\n",
    "num_obs = max([k for k in data['Date'].keys()])\n",
    "\n",
    "# Train the policy\n",
    "scores = reinforce(agent_policy,\n",
    "                   agent_optimizer,\n",
    "                   agent_hyperparameters[\"n_training_episodes\"], \n",
    "                   agent_hyperparameters[\"max_t\"],\n",
    "                   agent_hyperparameters[\"gamma\"], \n",
    "                   20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/all_data.csv\").to_dict() # full data with price and sentiment\n",
    "num_obs = max([k for k in data['Date'].keys()])\n",
    "\n",
    "env = Environment\n",
    "agent = Policy #Agent\n",
    "\n",
    "env_info = {'max' : num_obs , 'data':data}\n",
    "agent_info = {'model' : DecisionTreeRegressor(), 'cash':10000, 'crypto': ['BTC']}\n",
    "\n",
    "\n",
    "rl_glue = RLGlue(env, agent)  # Creates a new RLGlue experiment with the env and agent we chose \n",
    "rl_glue.rl_init(agent_info, env_info) # Pass RLGlue what it needs to initialize the agent and environment\n",
    "rl_glue.rl_start() \n",
    "\n",
    "\n",
    "num_steps = 11\n",
    "\n",
    "total_nupl = []\n",
    "actions = np.zeros(4)\n",
    "for i in range(num_steps):\n",
    "        reward, obs, action, done = rl_glue.rl_step()\n",
    "        print(\"iteration:\",i)\n",
    "        print(\"Total NUPL:\", reward)\n",
    "        print(\"New observation:\", obs[0], obs[1])\n",
    "        print(\"Agent Portfolio:\", action[0])\n",
    "        print(\"Action taken:\", action[1])\n",
    "\n",
    "        total_nupl.append(reward)\n",
    "        actions = np.vstack((actions,action[1]))\n",
    "        if done:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
